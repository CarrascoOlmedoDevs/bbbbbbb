# --- data_processor.py ---
import json
import os

def load_data(filepath):
    """Loads data from a JSON file. Returns None on error or invalid structure."""
    if not os.path.exists(filepath):
        print(f"Error: File not found at '{filepath}'")
        return None
    try:
        with open(filepath, 'r') as f:
            data = json.load(f)
        # Basic validation: ensure it's a dict and has a list named 'items'
        if not isinstance(data, dict) or 'items' not in data or not isinstance(data['items'], list):
             print(f"Error: Invalid data structure in '{filepath}'. Expected a dict with a list 'items'.")
             return None
        return data
    except json.JSONDecodeError:
        print(f"Error: Could not decode JSON from '{filepath}'. Check file format.")
        return None
    except Exception as e:
        print(f"An unexpected error occurred while loading data from '{filepath}': {e}")
        return None

def process_items(items_list, threshold):
    """Processes a list of items based on a numerical threshold.
    Each item is expected to be a dict with 'id' and 'value'.
    Returns a list of dictionaries with 'id' and 'status' ('processed' or 'skipped').
    Invalid items are skipped with a warning.
    """
    if not isinstance(items_list, list):
        print("Error: process_items expects a list as input.")
        return [] # Return empty list on invalid input

    results = []
    for item in items_list:
        # Validate item structure and value type
        if isinstance(item, dict) and 'id' in item and 'value' in item and isinstance(item['value'], (int, float)):
            if item['value'] > threshold:
                results.append({'id': item['id'], 'status': 'processed'})
            else:
                results.append({'id': item['id'], 'status': 'skipped'})
        else:
            # Handle items that don't match the expected format
            item_id = item.get('id', 'N/A') # Safely get ID for warning
            print(f"Warning: Skipping item with unexpected format or value type (ID: {item_id}, Item: {item}).")
            # Optionally, you could log or collect these invalid items separately
            # results.append({'item': item, 'status': 'invalid_format'}) # Example if you wanted to include them

    return results

# --- reporter.py ---

def generate_summary(processed_data):
    """Generates a summary count from processed data.
    Input is expected to be a list of dicts, each with a 'status' key.
    Returns a dictionary with counts for 'processed' and 'skipped'.
    """
    if not isinstance(processed_data, list):
        print("Error: generate_summary expects a list as input.")
        return {"processed": 0, "skipped": 0} # Return default counts on invalid input

    processed_count = 0
    skipped_count = 0
    # Note: If 'invalid_format' status was added in process_items, it would need handling here too.
    # Assuming only 'processed' and 'skipped' statuses are generated by process_items for valid inputs.
    for item in processed_data:
        # Validate item structure before accessing status
        if isinstance(item, dict) and 'status' in item:
            if item['status'] == 'processed':
                processed_count += 1
            elif item['status'] == 'skipped':
                skipped_count += 1
            # Else: status might be something else, ignore or log if necessary
        else:
             # This case should ideally not happen if process_items only returns valid structures,
             # but defensive programming is good.
             print(f"Warning: Item in processed_data has unexpected format or missing 'status': {item}")


    return {"processed": processed_count, "skipped": skipped_count}

def print_summary(summary_data):
    """Prints the summary data in a readable format.
    Input is expected to be a dictionary with 'processed' and 'skipped' keys.
    """
    # Basic validation for summary_data structure
    if not isinstance(summary_data, dict) or 'processed' not in summary_data or 'skipped' not in summary_data:
         print("Error: print_summary expects a dictionary with 'processed' and 'skipped' keys.")
         return

    print("\n--- Processing Summary ---")
    # Use .get for safety in case keys are unexpectedly missing (though validation above helps)
    print(f"  Processed Items: {summary_data.get('processed', 0)}")
    print(f"  Skipped Items: {summary_data.get('skipped', 0)}")
    print("------------------------\n")

# --- main_script.py ---
# This part orchestrates the process using functions defined above.

def main(filepath, threshold):
    """Main function to orchestrate data loading, processing, and reporting."""
    print(f"--- Starting Data Processing ---")
    print(f"Attempting to load data from: '{filepath}'")
    raw_data_container = load_data(filepath) # Call function from conceptual data_processor part

    if raw_data_container is None:
        print("Data loading failed. Aborting process.")
        return # Exit if data loading failed

    items_to_process = raw_data_container.get('items', []) # Safely get the list of items

    if not items_to_process:
        print("No items found in the data file to process.")
        # Still generate and print an empty summary
        summary_counts = generate_summary([])
        print_summary(summary_counts)
        return # Exit if no items

    print(f"Successfully loaded {len(items_to_process)} potential items.")
    print(f"Processing items with threshold value: {threshold}")
    processed_results = process_items(items_to_process, threshold) # Call function from conceptual data_processor part

    print("Generating processing summary...")
    summary_counts = generate_summary(processed_results) # Call function from conceptual reporter part

    print_summary(summary_counts) # Call function from conceptual reporter part
    print("--- Data Processing Complete ---")

# Example usage block
if __name__ == "__main__":
    # Define configuration parameters
    DATA_FILEPATH = 'data.json' # Input file path
    PROCESSING_THRESHOLD = 100 # Numerical threshold for processing

    # --- Optional: Create a dummy data.json file for demonstration ---
    # In a real application, this file would be provided externally.
    # This block makes the script runnable standalone for testing the flow.
    dummy_data_content_example = {
        "items": [
            {"id": 1, "value": 50},
            {"id": 2, "value": 120},
            {"id": 3, "value": 99},
            {"id": 4, "value": 150},
            {"id": 5, "value": "invalid_value"}, # Example of invalid data type
            {"id": 6, "value": 75},
            {"id": 7, "no_value_key": 200}, # Example of missing key
            {"id": 8, "value": 110}
        ],
        "other_info": "This is ignored"
    }
    # Check if the file exists before creating the dummy one
    if not os.path.exists(DATA_FILEPATH):
        print(f"'{DATA_FILEPATH}' not found. Creating a dummy data file for demonstration.")
        try:
            with open(DATA_FILEPATH, 'w') as f:
                json.dump(dummy_data_content_example, f, indent=4)
            print(f"Dummy data file created at '{DATA_FILEPATH}'.")
        except IOError as e:
            print(f"Error creating dummy data file: {e}")
            # Decide if you want to exit or continue without the file
            # For this example, we'll let main() handle the file not found error
    # --- End of Optional Dummy File Creation ---


    # Run the main process with defined parameters
    main(DATA_FILEPATH, PROCESSING_THRESHOLD)

    # --- Optional: Clean up the dummy data file ---
    # import os
    # if os.path.exists(DATA_FILEPATH) and 'dummy_data_content_example' in locals():
    #     # Only remove if we know we created it (basic check)
    #     try:
    #         os.remove(DATA_FILEPATH)
    #         print(f"Cleaned up dummy data file: '{DATA_FILEPATH}'")
    #     except OSError as e:
    #          print(f"Error removing dummy data file: {e}")
    # --- End of Optional Cleanup ---